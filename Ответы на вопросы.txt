Кейс 1
Привет!
Здесь предлагаю воспользоваться CTE. В первом CTE нужно отобрать пользователей, которые перешли по рекламе, (WHERE event = click)
Далее делаем еще одно CTE с событием (WHERE event = show) и считаем в рамках группы по user COUNT(*), соеденеям это множество с первым
табличным выражением. В результирующем SELECT-те считаем агрегирующей функцией AVG среднее кол-во событий, источником будет является второй CTE.

Кейс 2
Привет!
--Предлагаю воспользоваться аналитической функций COUNT(*)
SELECT *(необходимое кол-во полей),
--Делаем группировку по двуп полям (поле группы и поле параметра)
COUNT(*) OVER (PARTITION BY group_name, params) AS top
FROM table
--Далее, необходимо сделать сортировку от большего к меньшему, чтобы обратиться к полю, которое ты рассчитываешь в SELECT: нужно, либо указать его 
--порядковый номер, либо обернуть его другим SELECT * FROM (SELECT names_fiels FROM table) q ORDER BY field DESC
ORDER BY 2 
--Ограничиваем выборку 5 записями:
LIMIT 5 



Кейс 3
Привет, в проектах на работе код, как правило, можно найти в трех местах. И везде он храниться) 
Первое место у тебя на компьютере, когда ты разработал скрипт он сохранятеся (автоматом или в ручном режиме) у тебя в папке, где ты вел разработку.
После этого, как правило, мы отправляем все в удаленный репозиторий в git, на удаленном репозитории мы имеем копию кода, который у тебя на компьютере. 
В завершении, включается последний этап, его еще называют CI\CD, это когда код, который находиться в удаленном репозитории попадает на сервер, где этот
код будет выполняться. В маленьких проектах или PET-проектах git не используют, тогда код будет в двух местах - у тебя и на сервере который его
исполняет(его нужно будет туда закачать руками). У меня бывали случаи, когда я вел разработку и не было git и сервера, код испольнялся и хранился 
локально на рабочем компьютере, но это плохой поинт, так лучше не делать)

Кейс 4
Привет!
Эффективность работы с ошибками возрастает тем больше, чем больше ты их переделал) И не сдался)
Я, когда ловлю ошибку, первы делом, смотрю логи, куда печатается вывод исполняемого кода, будь то SQL, Python или Scala. В зависиомсти от инструмента,
вывод ошибок происходит по-разному. Например, в Python все ошибки мы можем увидеть в консоли Pycharm, если там ведем разработку. И Python, как правило, 
очень четко подсказывает, где что не так, указывает, на какой строке ошибка (не правельный тип, неверное название переменной и т.д.). Если это не 
помогло, тогда запускаю код в режиме
дебагинга (вот ссылка на youtube - https://www.youtube.com/watch?v=Lfqejg5hyDY, как это делается) и в таком режими ищу, где что не так.
По мере увеличения опыта, достаточно просто кинуть взгляд на код и уже видишь ошибку, для этого нужно просто время и
непрерывная разработка.

Кейс 5
Привет!
Да, у нас есть. Как правило ревью имеет 3 цели:
	1) Найти ошибку, исправить ее и не допустить код с багой в промышленную эксплуотацию;
	2) Человек, который проверяет, учиться на примерах свих коллег;
	3) Человек, которого проверяют, если есть ошибки, замечания по коду от коллег, учиться на этих комментариях и обратной связи.
Получается код-ревью - это непрерывный процесс развития команды и минимизация ошибок, а ошибки - это всегда дополнительные ресурсы на их исправление.
У нас код проходит проверку от двух коллег, после "Ок" от них, можно делать merge в master, цели такие как описал выши, они, мне кажется, везде 
одинаковые.


Кейс 6
Привет!
NiFi - это инструмент для загрузки и обработки данных между системами. Загрузку данных по концепции ETL или ELT можно сделать как с помощью
языка программирования (Python, Go, Scala, Java) DE, чаще всего использую Python и Scala,
так и с помощью разработнной программы, визуального инструмента, Informatica PC, SSIS, NiFi. Выбор инструмента зависит от задач и ресурсов компании.
Например, Informatica PC - это платный инструмент, тогда как NiFi можно использовать бесплатно. Так же, если на проекте используется Hadoop или
любое другое распределнное хранилище данных, то SSIS не сможет выполнить поставленные задачи, так как он работает только в рамках одной машины(сервера),
так же он не сможет выполнять потоковую обработку данных а NiFI сможет.


Кейс 7
Привет!
Как вариант, в БД можно класть сообщение, назначать ему слудующий ретрай тайм и сохранять количество попыток. Шедулер ходит по таблице и закидывает 
обратно в кафка нужные. Как настроить очередь поломаннх сообщений можно почитать здесь.
https://towardsdatascience.com/dead-letter-queue-dlq-in-kafka-29418e0ec6cf


Кейс 8
Привет!
Нужно эти параметры вынести в variables. 
Параметры в airflow (variables) — пары "key-value", хранимые в мета-БД Airflow. Они используются для хранения и получения произвольной информации из метабазы. 
Это могут быть, например, параметры конфигурации или список таблиц. Работать с Variables (создавать, обновлять, удалять) можно
через UI (Admin -> Variables), где можно прописать пары "key-value" явно или загрузить json-файл, а также можно использовать airflow variables в CLI
(https://airflow.apache.org/docs/apache-airflow/stable/cli-and-env-variables-ref.html?highlight=variables#variables). 
Примеры с Variables можно посмотреть здесь (https://www.applydatascience.com/airflow/airflow-variables/).
Получить Variable в коде DAG'а можно так:
from airflow.models import Variable
foo = Variable.get('foo')


Кейс 9
Привет!
Сперва, я бы уточнил у бизнес-заказчика или аналитика кто делал постановку задания, бизнес требования к этому кейсу, так как мы можем чего-то не
знать, а лучше не придумывать самостоятельно требования. Если коллеги не смогут ответить и спросят у нас, что мы можем предложить, варианты могут быть следующие:
	1)минимальное, максимальное, среднее, медианное значение по столбцу, ноль (это если числовой столбец);
	2)значение по бизнес требованию, тут любое значение подставляем;
	3)значение на основе дополнительной аналитики (можно провести доп аналитику и посмотреть на данные под разными углом и выявить закономерности);
	4)удалить строчки с пустыми значениями в необходимых столбцах.

Что касается технической части, то посмотри(почитай) про следующие методы:
	1)fillna() на конкретные числовые значения;
	2)dropna() удаление записей;
	3)isnull() генерирует булеву маску для отсутствующих значений.
